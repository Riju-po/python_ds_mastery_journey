





# In 6.3.1_scaling_and_optimization.ipynb
import requests
import os

# URL for a single month of Yellow Taxi trip data (e.g., Jan 2023)
# File size is ~300-400 MB
url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
filename = "nyc_taxi_jan_2023.parquet"

if not os.path.exists(filename):
    print(f"Downloading {filename}...")
    response = requests.get(url)
    if response.status_code == 200:
        with open(filename, "wb") as f:
            f.write(response.content)
        print("Download complete.")
    else:
        print(f"Failed to download file. Status code: {response.status_code}")
else:
    print(f"'{filename}' already exists.")





import pandas as pd
import numpy as np

# Load the data with Pandas to check initial memory usage
# Note: We are loading a Parquet file, which is already more efficient than CSV!
df_pandas = pd.read_parquet("nyc_taxi_jan_2023.parquet")

# Check initial memory usage
print("--- Initial Pandas Memory Usage ---")
df_pandas.info(memory_usage='deep')

# --- Strategy 1: Downcasting Numerical Columns ---
# Convert int64/float64 to smaller types if the value range allows
df_optimized = df_pandas.copy()
for col in df_optimized.select_dtypes(include=['int', 'float']).columns:
    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='integer')
    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='float')

# --- Strategy 2: Converting Object Columns to Category ---
# Best for columns with low cardinality (few unique values)
for col in ['VendorID', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type']:
    if col in df_optimized.columns:
        df_optimized[col] = df_optimized[col].astype('category')

print("\n--- Optimized Pandas Memory Usage ---")
df_optimized.info(memory_usage='deep')

# --- Strategy 3: Reading in Chunks (for files too big to load at all) ---
# This is how you would process a massive CSV file that doesn't fit in RAM.
# We'll simulate this conceptually.
# chunk_iter = pd.read_csv("massive_file.csv", chunksize=100000)
# for chunk in chunk_iter:
#     # Process each chunk here (e.g., calculate aggregations)
#     print(f"Processing a chunk of size {len(chunk)}")





# We already loaded from Parquet. Let's save our optimized DataFrame to compare.
# Note: You need pyarrow installed for this.

# Save to Parquet
df_optimized.to_parquet("optimized_taxi_data.parquet")

# Save to Feather
df_optimized.to_feather("optimized_taxi_data.feather")

print("\n--- File Sizes on Disk ---")
print(f"Original Parquet: {os.path.getsize('nyc_taxi_jan_2023.parquet') / 1e6:.2f} MB")
print(f"Optimized Parquet: {os.path.getsize('optimized_taxi_data.parquet') / 1e6:.2f} MB")
print(f"Optimized Feather: {os.path.getsize('optimized_taxi_data.feather') / 1e6:.2f} MB")
# To see the real difference, let's save a CSV
# df_optimized.to_csv("optimized_taxi_data.csv", index=False)
# print(f"Optimized CSV: {os.path.getsize('optimized_taxi_data.csv') / 1e6:.2f} MB") # This will be much larger!





import dask.dataframe as dd

# Read the data with Dask. It reads it lazily.
# npartitions can be set to the number of cores you have.
ddf = dd.read_parquet("nyc_taxi_jan_2023.parquet")
print("\n--- Dask DataFrame ---")
print(ddf) # Note it doesn't show data, just the structure.

# Operations look just like Pandas, but they are lazy
mean_trip_distance = ddf['trip_distance'].mean()
print(f"\nMean trip distance (lazy object): {mean_trip_distance}")

# To get the result, we call .compute()
print("Computing mean trip distance with Dask...")
result = mean_trip_distance.compute()
print(f"Result: {result:.2f}")





# Conceptual example (no need to run a full training)
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# When training, this would use all cores to build trees in parallel:
# rf = RandomForestClassifier(n_estimators=500, n_jobs=-1)

# When tuning, this would run CV folds in parallel:
# grid_search = GridSearchCV(estimator=rf, param_grid=my_params, n_jobs=-1)
print("\n--- joblib Example ---")
print("Using 'n_jobs=-1' in Scikit-learn parallelizes training and tuning.")











import time
import pandas as pd
import dask.dataframe as dd

# --- Pandas Attempt ---
print("\n--- Timing Pandas ---")
start = time.time()
df_pandas = pd.read_parquet("nyc_taxi_jan_2023.parquet")
pandas_result = df_pandas.groupby("payment_type")["tip_amount"].mean()
end = time.time()
print("Pandas groupby result:")
print(pandas_result)
print(f"Pandas execution time: {end - start:.2f} seconds")

# --- Dask Attempt ---
print("\n--- Timing Dask ---")
start = time.time()
ddf = dd.read_parquet("nyc_taxi_jan_2023.parquet")
dask_aggregation = ddf.groupby("payment_type")["tip_amount"].mean()
dask_result = dask_aggregation.compute()
end = time.time()
print("Dask groupby result:")
print(dask_result)
print(f"Dask execution time: {end - start:.2f} seconds")







