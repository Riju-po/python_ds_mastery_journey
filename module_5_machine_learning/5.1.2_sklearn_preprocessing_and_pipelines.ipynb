{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f13f8ee-d168-4cee-a755-032421adc731",
   "metadata": {},
   "source": [
    "## üîß Why Preprocessing is Essential\n",
    "\n",
    "Many machine learning algorithms ‚Äî especially **linear models**, **SVMs**, and **neural networks** ‚Äî are sensitive to the **scale of the input features**.\n",
    "\n",
    "üìå For example:\n",
    "- If one feature ranges from `0 to 1` and another ranges from `0 to 1,000,000`, the algorithm might incorrectly assume that the second feature is more important, just because it has a larger scale.\n",
    "\n",
    "‚úÖ To address this:\n",
    "- We **normalize** (scale values between 0 and 1) or **standardize** (mean = 0, std = 1) our data to ensure all features contribute equally to the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "Additionally, most machine learning algorithms require all inputs to be **numeric**.\n",
    "\n",
    "üß† That means:\n",
    "- Categorical variables like `\"Male\"/\"Female\"` or `\"New York\"/\"Chicago\"` must be **converted to numbers** using encoding techniques such as:\n",
    "  - **One-Hot Encoding**\n",
    "  - **Label Encoding**\n",
    "  - **Ordinal Encoding**\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Without preprocessing**, our models might:\n",
    "- Make wrong assumptions about feature importance\n",
    "- Fail to process non-numeric data\n",
    "- Yield poor performance or convergence issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69998086-6f35-4d8b-b5ae-10782e12b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Let's create a more realistic, messy dataset\n",
    "data = {\n",
    "    'age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
    "    'salary': [50000, 54000, 60000, 68000, 75000, 80000, 90000, 110000],\n",
    "    'city': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'Chicago', 'New York', 'Chicago'],\n",
    "    'purchased': [0, 1, 0, 0, 1, 1, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['age', 'salary', 'city']]\n",
    "y = df['purchased']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c8238-ca88-417a-a1f9-eb136dbd43af",
   "metadata": {},
   "source": [
    "## üîÑ Transformers and Preprocessing\n",
    "\n",
    "Scikit-learn's preprocessing tools follow the same **`fit` / `transform` API** as models.\n",
    "\n",
    "### üìå Core Methods\n",
    "\n",
    "- **`fit(data)`**  \n",
    "  The transformer \"learns\" the necessary parameters from the data.  \n",
    "  - Example:  \n",
    "    - `StandardScaler` learns the **mean** and **standard deviation**.\n",
    "    - `OneHotEncoder` learns the **unique categories**.\n",
    "  - ‚ö†Ô∏è You should **only fit on the training data**.\n",
    "\n",
    "- **`transform(data)`**  \n",
    "  Applies the learned transformation to the data.  \n",
    "  ‚úÖ Use this on **both training and testing data**.\n",
    "\n",
    "- **`fit_transform(data)`**  \n",
    "  A shortcut that performs both `fit` and `transform`.  \n",
    "  ‚úÖ Use only on **training data**.\n",
    "\n",
    "---\n",
    "\n",
    "## A. üî¢ Scaling Numerical Features\n",
    "\n",
    "Scaling ensures that features contribute equally to the model by bringing them to the same scale.\n",
    "\n",
    "### üß™ `StandardScaler`\n",
    "- **Standardizes** features by removing the mean and scaling to unit variance.\n",
    "- The resulting distribution will have:\n",
    "  - Mean = 0\n",
    "  - Standard Deviation = 1\n",
    "- Best suited when data follows a normal distribution.\n",
    "\n",
    "### üìè `MinMaxScaler`\n",
    "- Scales features to a **specific range**, typically [0, 1].\n",
    "- Preserves the **shape** of the original distribution.\n",
    "- Useful when features have **varying units or scales**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Scaling is especially important for models like:\n",
    "- Linear Regression\n",
    "- SVM\n",
    "- KNN\n",
    "- Neural Networks\n",
    "\n",
    "Without scaling, features with larger magnitudes can **dominate** the learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d80a59-6d12-4ead-bc37-82f4fc9da7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Standard Scaled Training Data ---\n",
      "[[-1.55563492 -1.28917835]\n",
      " [ 1.41421356  1.74418248]\n",
      " [-0.70710678 -0.78361822]\n",
      " [ 0.14142136 -0.02527801]\n",
      " [-0.28284271 -0.3791701 ]\n",
      " [ 0.98994949  0.7330622 ]]\n",
      "Mean: [-2.03540888e-16  0.00000000e+00]\n",
      "Std Dev: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Separate numerical and categorical columns for now\n",
    "X_train_num = X_train[['age', 'salary']]\n",
    "X_test_num = X_test[['age', 'salary']]\n",
    "\n",
    "# --- StandardScaler ---\n",
    "scaler = StandardScaler()\n",
    "# Fit on the training data ONLY to learn the mean and std dev\n",
    "scaler.fit(X_train_num)\n",
    "# Transform both train and test data\n",
    "X_train_scaled = scaler.transform(X_train_num)\n",
    "X_test_scaled = scaler.transform(X_test_num)\n",
    "\n",
    "print(\"--- Standard Scaled Training Data ---\")\n",
    "print(X_train_scaled)\n",
    "print(f\"Mean: {X_train_scaled.mean(axis=0)}\") # Should be close to 0\n",
    "print(f\"Std Dev: {X_train_scaled.std(axis=0)}\")   # Should be close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554e91c-4d7c-47b0-8de9-e8365b164d8a",
   "metadata": {},
   "source": [
    "## B. üß© Encoding Categorical Features\n",
    "\n",
    "When working with categorical data, we need to convert it into a numeric format because most machine learning models can‚Äôt handle strings directly.\n",
    "\n",
    "### üéØ OneHotEncoder\n",
    "\n",
    "- Converts categorical variables into a **\"one-hot\" encoded format**.\n",
    "- It creates a **new binary column** for each category.\n",
    "- Each row will have a `1` in the column corresponding to its category and `0` elsewhere.\n",
    "- This is the **preferred method** for **nominal** (unordered) categorical variables.\n",
    "\n",
    "#### ‚úÖ Example\n",
    "\n",
    "| City        | Chicago | Los Angeles | New York |\n",
    "|-------------|---------|-------------|----------|\n",
    "| New York    | 0       | 0           | 1        |\n",
    "| Los Angeles | 0       | 1           | 0        |\n",
    "| Chicago     | 1       | 0           | 0        |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ† Why One-Hot Encoding?\n",
    "\n",
    "- Prevents the model from assuming **ordinal relationships** between categories.\n",
    "- Keeps distance-based algorithms like **KNN** or **SVM** from being misled.\n",
    "\n",
    "‚ö†Ô∏è Note:  \n",
    "OneHotEncoder can produce many columns if the category has many unique values. This is known as the **curse of dimensionality**, so use carefully with high-cardinality features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9091d049-bfaa-4741-824e-626d650d60a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- One-Hot Encoded Training Data ---\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Separate the categorical column\n",
    "X_train_cat = X_train[['city']]\n",
    "X_test_cat = X_test[['city']]\n",
    "\n",
    "# --- OneHotEncoder ---\n",
    "# handle_unknown='ignore' tells it to ignore categories in the test set that weren't in the training set\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "# Fit on training data to learn the categories ('New York', 'Los Angeles', 'Chicago')\n",
    "ohe.fit(X_train_cat)\n",
    "# Transform both\n",
    "X_train_ohe = ohe.transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "print(\"\\n--- One-Hot Encoded Training Data ---\")\n",
    "# The columns correspond to the learned categories: ['Chicago', 'Los Angeles', 'New York']\n",
    "print(X_train_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb8d76-b449-4766-990e-f188694a6d6d",
   "metadata": {},
   "source": [
    "- This manual process of separating columns, fitting, and transforming is tedious and prone to errors. This is why we use Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c7e52-5e86-46e4-adb4-41615959b0fe",
   "metadata": {},
   "source": [
    "## üîó 2. Pipelines: Streamlining the Workflow\n",
    "\n",
    "A **Pipeline** is a way to **chain multiple steps together** ‚Äî like data transformations (scaling, encoding) and the final machine learning model ‚Äî into **one single object**.\n",
    "\n",
    "### üõ† How It Works\n",
    "\n",
    "- When you call `.fit()` on a pipeline:\n",
    "  - It fits all the transformers (like scalers or encoders) in order.\n",
    "  - Then it fits the final estimator (your model).\n",
    "\n",
    "- When you call `.predict()`:\n",
    "  - It first transforms the input data using the fitted transformers.\n",
    "  - Then it makes predictions using the trained model.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why Pipelines are Useful\n",
    "\n",
    "- üîí **Prevents data leakage**  \n",
    "  (Example: accidentally fitting the scaler on test data).\n",
    "\n",
    "- üßπ **Cleaner and simpler code**  \n",
    "  No need to manually fit and transform step-by-step.\n",
    "\n",
    "- üì¶ **Reusable and exportable**  \n",
    "  You can save the entire pipeline and load it later to make predictions on new data.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Need to Transform Different Columns Differently?\n",
    "\n",
    "We use **`ColumnTransformer`** when:\n",
    "- Some columns need scaling (numeric).\n",
    "- Some columns need encoding (categorical).\n",
    "\n",
    "It lets us **apply different preprocessing steps to different columns** in a structured way ‚Äî all inside the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8740677e-fa9d-451c-bbc6-cbd0a4b9adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pipeline has been trained! ---\n",
      "\n",
      "Pipeline Accuracy on Test Set: 0.5000\n",
      "\n",
      "Prediction for new data point: Not Purchased\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Redefine our full train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# 1. Define the preprocessing steps for numerical and categorical features\n",
    "# The first element is a name, the second is the transformer, the third is the list of columns\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 2. Create the preprocessor object with ColumnTransformer\n",
    "# This applies the right transformer to the right columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, ['age', 'salary']),\n",
    "        ('cat', categorical_transformer, ['city'])\n",
    "    ])\n",
    "\n",
    "# 3. Create the full pipeline\n",
    "# It has two steps: 'preprocessor' and 'classifier'\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# 4. Now, train the entire pipeline on the raw training data!\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"\\n--- Pipeline has been trained! ---\")\n",
    "\n",
    "# 5. Make predictions and evaluate\n",
    "# The pipeline handles all the transformations for X_test automatically\n",
    "accuracy = model_pipeline.score(X_test, y_test)\n",
    "print(f\"\\nPipeline Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "# You can even make predictions on new, raw data\n",
    "new_data = pd.DataFrame([{'age': 38, 'salary': 72000, 'city': 'Los Angeles'}])\n",
    "prediction = model_pipeline.predict(new_data)\n",
    "print(f\"\\nPrediction for new data point: {'Purchased' if prediction[0] else 'Not Purchased'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59915ea-19fa-48ed-bded-d31464ceb8fb",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8baf932-1bf7-4695-af2f-418ba79a12c3",
   "metadata": {},
   "source": [
    "**1. Manual Preprocessing:**\n",
    "- Using the titanic dataset (sns.load_dataset('titanic')).\n",
    "- Create X from the columns pclass, sex, age, and fare. Create y from the survived column.\n",
    "- Handle the missing age values by filling them with the median.\n",
    "- Perform a train-test split (test_size=0.3, random_state=42).\n",
    "- On the training and testing sets separately:\n",
    "    - One-hot encode the sex column.\n",
    "    - Scale the age and fare columns using StandardScaler.\n",
    "    - Combine the preprocessed numerical and categorical features back into a final X_train_processed and X_test_processed. (Hint: use np.hstack).\n",
    "- Train a LogisticRegression model on the processed training data and evaluate its accuracy on the processed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "878cf0c6-710b-45a5-9c91-21627b836041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (623, 4)\n",
      "Testing set shape: (268, 4)\n",
      "\n",
      "Applying One-Hot Encoding to the 'sex' column...\n",
      "One-Hot Encoded feature names:['sex_female' 'sex_male']\n",
      "\n",
      "Final processed training set shape: (623, 5)\n",
      "Final processed testing set shape: (268, 5)\n",
      "\n",
      "\n",
      "--- Model Evaluation ---\n",
      "Accuracy on the processed test data: 0.7836\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "titanic_df=sns.load_dataset('titanic')\n",
    "\n",
    "features= ['pclass', 'sex', 'age', 'fare']\n",
    "target = 'survived'\n",
    "X= titanic_df[features]\n",
    "y=titanic_df[target]\n",
    "\n",
    "# Handle the missing age values by filling them with the median\n",
    "age_median=X['age'].median()\n",
    "X.loc[X['age'].isnull(), 'age'] = age_median\n",
    "\n",
    "# Perform a train-test split (test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42, stratify =y)\n",
    "\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\\n\")\n",
    "\n",
    "categorical_features= ['sex']\n",
    "numerical_features = ['age', 'fare', 'pclass']\n",
    "\n",
    "# One-hot encode the sex column\n",
    "print(\"Applying One-Hot Encoding to the 'sex' column...\")\n",
    "ohe =OneHotEncoder(sparse_output = False, handle_unknown='ignore')\n",
    "\n",
    "X_train_sex_encoded = ohe.fit_transform(X_train[categorical_features])\n",
    "X_test_sex_encoded = ohe.transform(X_test[categorical_features])\n",
    "print(f\"One-Hot Encoded feature names:{ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Scale the age and fare columns using StandardScaler\n",
    "scaler= StandardScaler()\n",
    "\n",
    "X_train_numerical_scaled = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test_numerical_scaled =scaler.transform(X_test[numerical_features])\n",
    "\n",
    "# Combine the preprocessed numerical and categorical features back into a final X_train_processed and X_test_processed\n",
    "X_train_processed = np.hstack((X_train_numerical_scaled, X_train_sex_encoded))\n",
    "X_test_processed = np.hstack((X_test_numerical_scaled, X_test_sex_encoded))\n",
    "\n",
    "print(f\"\\nFinal processed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Final processed testing set shape: {X_test_processed.shape}\\n\")\n",
    "\n",
    "# Train a LogisticRegression model on the processed training data and evaluate its accuracy on the processed test data\n",
    "model = LogisticRegression(random_state= 42)\n",
    "model.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred= model.predict(X_test_processed)\n",
    "accuracy= accuracy_score(y_test, y_pred)\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Accuracy on the processed test data: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba045d2d-155e-4e99-bb02-fecd658b59fc",
   "metadata": {},
   "source": [
    "**2. Building a Pipeline (The \"Amaze Factor\" Way):**\n",
    "- Using the same titanic data and the same X and y as Exercise 1.\n",
    "- This time, do not manually fill the missing age values. We will handle this in the pipeline.\n",
    "- Perform a train-test split on the raw data.\n",
    "- Create a pipeline that performs the following steps in order:\n",
    "1. A ColumnTransformer that:\n",
    "    - Applies a SimpleImputer(strategy='median') and a StandardScaler to the numerical columns (age, fare). (Hint: you'll need to create a numeric_pipeline using Pipeline for this).\n",
    "    - Applies a OneHotEncoder to the categorical columns (pclass, sex).\n",
    "2. A final LogisticRegression classifier.\n",
    "- Train this single pipeline object on your raw training data.\n",
    "- Evaluate the pipeline's accuracy on the raw test data.\n",
    "- In a Markdown cell, compare the simplicity and robustness of the pipeline approach (Exercise 2) to the manual approach (Exercise 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d75d876-77d3-4751-ae0e-0382a2cd28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw training set shape: (623, 4)\n",
      "Raw testing set shape: (268, 4)\n",
      "\n",
      "Accuracy of the pipeline on the test data: 0.7873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Perform a train-test split (test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 42, stratify =y)\n",
    "\n",
    "print(f\"\\nRaw training set shape: {X_train.shape}\")\n",
    "print(f\"Raw testing set shape: {X_test.shape}\\n\")\n",
    "\n",
    "numerical_features= ['age', 'fare']\n",
    "categorical_features = ['pclass', 'sex']\n",
    "\n",
    "numeric_transformer= Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer= Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "model_pipeline= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "y_pred= model_pipeline.predict(X_test)\n",
    "\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the pipeline on the test data: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec5bd4-482d-4715-b9a6-3b9c27956ac6",
   "metadata": {},
   "source": [
    "- **Simplicity:** The manual approach is messy with many separate steps. A pipeline organizes everything into one clean, readable object.\n",
    "\n",
    "- **Safety:** It's easy to make mistakes and \"leak\" test data during manual preprocessing, which leads to misleadingly high accuracy scores. A pipeline automatically prevents this, ensuring a more honest evaluation.\n",
    "\n",
    "- **Robustness:** A pipeline is a single, reusable object that's easy to save, modify, and use in a real application. The manual process is brittle and hard to manage.\n",
    "\n",
    "In short, the pipeline approach is the professional standard because it's cleaner, safer, and more reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
