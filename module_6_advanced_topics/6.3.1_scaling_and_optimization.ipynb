{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb20aef0-17c7-4a08-9da0-9baae66f5c48",
   "metadata": {},
   "source": [
    "# Module 6.3: Working with Larger Datasets & Basic Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6494a2-4daa-4fec-81b3-1a339e20a496",
   "metadata": {},
   "source": [
    "**Dataset for this module:**\n",
    "\n",
    "We need a large dataset to demonstrate these concepts. **The NYC Taxi & Limousine Commission dataset** is a perfect real-world example. We will download one month of data, which is large enough to be slow in Pandas but manageable for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ff397d-53ba-41da-9486-2b7e465270ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading nyc_taxi_jan_2023.parquet...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "# In 6.3.1_scaling_and_optimization.ipynb\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# URL for a single month of Yellow Taxi trip data (e.g., Jan 2023)\n",
    "# File size is ~300-400 MB\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "filename = \"nyc_taxi_jan_2023.parquet\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"'{filename}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8bdb3-9e06-415f-b17c-192a03e99ce8",
   "metadata": {},
   "source": [
    "# Topic 1: Pandas Memory Optimization\n",
    "Before switching to other libraries, let's see how much we can optimize Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd008d78-bd3f-4c95-aed6-6be718998233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Pandas Memory Usage ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3066766 entries, 0 to 3066765\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int64         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int64         \n",
      " 8   DOLocationID           int64         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  airport_fee            float64       \n",
      "dtypes: datetime64[us](2), float64(12), int64(4), object(1)\n",
      "memory usage: 565.6 MB\n",
      "\n",
      "--- Optimized Pandas Memory Usage ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3066766 entries, 0 to 3066765\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               category      \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float32       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             category      \n",
      " 6   store_and_fwd_flag     category      \n",
      " 7   PULocationID           category      \n",
      " 8   DOLocationID           category      \n",
      " 9   payment_type           category      \n",
      " 10  fare_amount            float32       \n",
      " 11  extra                  float32       \n",
      " 12  mta_tax                float32       \n",
      " 13  tip_amount             float32       \n",
      " 14  tolls_amount           float32       \n",
      " 15  improvement_surcharge  float32       \n",
      " 16  total_amount           float32       \n",
      " 17  congestion_surcharge   float32       \n",
      " 18  airport_fee            float32       \n",
      "dtypes: category(6), datetime64[us](2), float32(10), float64(1)\n",
      "memory usage: 210.6 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data with Pandas to check initial memory usage\n",
    "# Note: We are loading a Parquet file, which is already more efficient than CSV!\n",
    "df_pandas = pd.read_parquet(\"nyc_taxi_jan_2023.parquet\")\n",
    "\n",
    "# Check initial memory usage\n",
    "print(\"--- Initial Pandas Memory Usage ---\")\n",
    "df_pandas.info(memory_usage='deep')\n",
    "\n",
    "# --- Strategy 1: Downcasting Numerical Columns ---\n",
    "# Convert int64/float64 to smaller types if the value range allows\n",
    "df_optimized = df_pandas.copy()\n",
    "for col in df_optimized.select_dtypes(include=['int', 'float']).columns:\n",
    "    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='integer')\n",
    "    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='float')\n",
    "\n",
    "# --- Strategy 2: Converting Object Columns to Category ---\n",
    "# Best for columns with low cardinality (few unique values)\n",
    "for col in ['VendorID', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type']:\n",
    "    if col in df_optimized.columns:\n",
    "        df_optimized[col] = df_optimized[col].astype('category')\n",
    "\n",
    "print(\"\\n--- Optimized Pandas Memory Usage ---\")\n",
    "df_optimized.info(memory_usage='deep')\n",
    "\n",
    "# --- Strategy 3: Reading in Chunks (for files too big to load at all) ---\n",
    "# This is how you would process a massive CSV file that doesn't fit in RAM.\n",
    "# We'll simulate this conceptually.\n",
    "# chunk_iter = pd.read_csv(\"massive_file.csv\", chunksize=100000)\n",
    "# for chunk in chunk_iter:\n",
    "#     # Process each chunk here (e.g., calculate aggregations)\n",
    "#     print(f\"Processing a chunk of size {len(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64562a12-0e48-494f-b680-f29e703e47e9",
   "metadata": {},
   "source": [
    "# Topic 2: Efficient Data Storage Formats (Parquet, Feather)\n",
    "\n",
    "The format you save your data in matters.\n",
    "\n",
    "---\n",
    "\n",
    "### CSV\n",
    "- **Type**: Text-based  \n",
    "- **Pros**: Simple, universally supported  \n",
    "- **Cons**: Slow to parse, large file size  \n",
    "\n",
    "---\n",
    "\n",
    "### Parquet\n",
    "- **Type**: Binary, columnar format  \n",
    "- **Pros**:  \n",
    "  - Highly compressed  \n",
    "  - Preserves data types  \n",
    "  - Much faster to read (especially when selecting only a subset of columns)  \n",
    "- **Use Case**: Industry standard for **big data**  \n",
    "\n",
    "---\n",
    "\n",
    "### Feather\n",
    "- **Type**: Binary format  \n",
    "- **Pros**:  \n",
    "  - Designed for maximum **read/write speed** between Python (Pandas) and R  \n",
    "- **Cons**:  \n",
    "  - Not as compressed as Parquet  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d096606c-7ea5-4597-8b54-1a1b266dc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- File Sizes on Disk ---\n",
      "Original Parquet: 47.67 MB\n",
      "Optimized Parquet: 63.07 MB\n",
      "Optimized Feather: 86.43 MB\n"
     ]
    }
   ],
   "source": [
    "# We already loaded from Parquet. Let's save our optimized DataFrame to compare.\n",
    "# Note: You need pyarrow installed for this.\n",
    "\n",
    "# Save to Parquet\n",
    "df_optimized.to_parquet(\"optimized_taxi_data.parquet\")\n",
    "\n",
    "# Save to Feather\n",
    "df_optimized.to_feather(\"optimized_taxi_data.feather\")\n",
    "\n",
    "print(\"\\n--- File Sizes on Disk ---\")\n",
    "print(f\"Original Parquet: {os.path.getsize('nyc_taxi_jan_2023.parquet') / 1e6:.2f} MB\")\n",
    "print(f\"Optimized Parquet: {os.path.getsize('optimized_taxi_data.parquet') / 1e6:.2f} MB\")\n",
    "print(f\"Optimized Feather: {os.path.getsize('optimized_taxi_data.feather') / 1e6:.2f} MB\")\n",
    "# To see the real difference, let's save a CSV\n",
    "# df_optimized.to_csv(\"optimized_taxi_data.csv\", index=False)\n",
    "# print(f\"Optimized CSV: {os.path.getsize('optimized_taxi_data.csv') / 1e6:.2f} MB\") # This will be much larger!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd8a1f-4b90-4ecb-b884-5515e0ce93a1",
   "metadata": {},
   "source": [
    "# Topic 3: Dask - Parallel Computing with DataFrames\n",
    "\n",
    "### Concept\n",
    "Dask is a **parallel computing library** that scales your existing Python ecosystem.  \n",
    "- A **Dask DataFrame** is a collection of smaller **Pandas DataFrames (partitions)**.  \n",
    "- These partitions can be processed **in parallel** across your CPU cores.  \n",
    "\n",
    "---\n",
    "\n",
    "### Lazy Evaluation\n",
    "- Dask builds a **\"task graph\"** of your operations.  \n",
    "- Nothing is actually computed until you explicitly call **`.compute()`**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ed398b-50e3-4c2e-9d58-a95ac4700440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dask DataFrame ---\n",
      "Dask DataFrame Structure:\n",
      "              VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type fare_amount    extra  mta_tax tip_amount tolls_amount improvement_surcharge total_amount congestion_surcharge airport_fee\n",
      "npartitions=1                                                                                                                                                                                                                                                                                 \n",
      "                 int64       datetime64[us]        datetime64[us]         float64       float64    float64             string        int64        int64        int64     float64  float64  float64    float64      float64               float64      float64              float64     float64\n",
      "                   ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...\n",
      "Dask Name: read_parquet, 1 expression\n",
      "Expr=ReadParquetFSSpec(bcc0d4e)\n",
      "\n",
      "Mean trip distance (lazy object): <dask_expr.expr.Scalar: expr=ReadParquetFSSpec(bcc0d4e)['trip_distance'].mean(), dtype=float64>\n",
      "Computing mean trip distance with Dask...\n",
      "Result: 3.85\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read the data with Dask. It reads it lazily.\n",
    "# npartitions can be set to the number of cores you have.\n",
    "ddf = dd.read_parquet(\"nyc_taxi_jan_2023.parquet\")\n",
    "print(\"\\n--- Dask DataFrame ---\")\n",
    "print(ddf) # Note it doesn't show data, just the structure.\n",
    "\n",
    "# Operations look just like Pandas, but they are lazy\n",
    "mean_trip_distance = ddf['trip_distance'].mean()\n",
    "print(f\"\\nMean trip distance (lazy object): {mean_trip_distance}\")\n",
    "\n",
    "# To get the result, we call .compute()\n",
    "print(\"Computing mean trip distance with Dask...\")\n",
    "result = mean_trip_distance.compute()\n",
    "print(f\"Result: {result:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6568b-8b80-481f-af4c-82a996e5794a",
   "metadata": {},
   "source": [
    "# Topic 4: joblib - Parallelizing Scikit-learn\n",
    "\n",
    "### Concept\n",
    "Joblib provides the **simplest form of parallelization** in Scikit-learn.  \n",
    "Many Scikit-learn algorithms and utilities have an **`n_jobs`** parameter.  \n",
    "\n",
    "---\n",
    "\n",
    "### n_jobs = -1\n",
    "- Tells Scikit-learn to use **all available CPU cores** to parallelize the task.  \n",
    "\n",
    "---\n",
    "\n",
    "### How it works\n",
    "- **RandomForestClassifier**:  \n",
    "  Trains the different trees in the forest on **different CPU cores simultaneously**.  \n",
    "\n",
    "- **GridSearchCV**:  \n",
    "  Evaluates different hyperparameter combinations on **different cores in parallel**.  \n",
    "\n",
    "ðŸ‘‰ This can lead to **massive speedups** in training and model selection.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "663c8deb-5ed3-4647-80b2-ccc9b8735bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- joblib Example ---\n",
      "Using 'n_jobs=-1' in Scikit-learn parallelizes training and tuning.\n"
     ]
    }
   ],
   "source": [
    "# Conceptual example (no need to run a full training)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# When training, this would use all cores to build trees in parallel:\n",
    "# rf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "\n",
    "# When tuning, this would run CV folds in parallel:\n",
    "# grid_search = GridSearchCV(estimator=rf, param_grid=my_params, n_jobs=-1)\n",
    "print(\"\\n--- joblib Example ---\")\n",
    "print(\"Using 'n_jobs=-1' in Scikit-learn parallelizes training and tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ce01e-664c-4a6d-a244-9f10fc4e6e1e",
   "metadata": {},
   "source": [
    "# Topic 5: Apache Spark (PySpark) - Conceptual Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf29416-2088-4444-a420-26ed59a74b76",
   "metadata": {},
   "source": [
    "### An Overview of Apache Spark (PySpark)\n",
    "\n",
    "*   **What it is:** Apache Spark is a powerful, open-source, **distributed computing system** designed for big data processing and analytics. PySpark is the Python API for Spark.\n",
    "\n",
    "*   **When it's needed:** Spark is the tool of choice when your data is truly **\"Big Data\"**â€”typically hundreds of gigabytes, terabytes, or even petabytesâ€”and is stored across a **cluster of multiple machines**. It is not designed to be used on a single laptop for datasets that Dask can handle.\n",
    "\n",
    "*   **Dask vs. Spark (\"Amaze Factor\" explanation):**\n",
    "    *   **Scale:** Dask is designed to scale Python code (like Pandas and Scikit-learn) from a single core to multiple cores on **one machine**, or to a small/medium-sized cluster. Spark is designed from the ground up for massive, multi-machine clusters.\n",
    "    *   **Ecosystem:** Dask is a lightweight, pure Python library that integrates directly with the existing Python data science ecosystem. Spark is a more heavyweight, self-contained ecosystem written in Scala (running on the JVM), which has its own libraries for dataframes, machine learning (MLlib), etc.\n",
    "    *   **Use Case:** If you love the Pandas API and your data is in the 1-100 GB range, Dask is often the more natural and easier choice to scale your work on your local machine or a single server. If your organization's data lives in a massive, distributed data lake and you need to run complex ETL and ML jobs across a large cluster, Spark is the industry standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018211d-5ae2-4701-9f2b-34a038b2e526",
   "metadata": {},
   "source": [
    "# Mini-Project: Dask on a Large CSV\n",
    "Goal: Compare the performance of Pandas vs. Dask for a basic aggregation task on our large taxi dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9438e9f7-18ab-48c8-80b3-333c646e45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Timing Pandas ---\n",
      "Pandas groupby result:\n",
      "payment_type\n",
      "0    3.733109\n",
      "1    4.170799\n",
      "2    0.001675\n",
      "3    0.029469\n",
      "4    0.051490\n",
      "Name: tip_amount, dtype: float64\n",
      "Pandas execution time: 0.85 seconds\n",
      "\n",
      "--- Timing Dask ---\n",
      "Dask groupby result:\n",
      "payment_type\n",
      "0    3.733109\n",
      "1    4.170799\n",
      "2    0.001675\n",
      "3    0.029469\n",
      "4    0.051490\n",
      "Name: tip_amount, dtype: float64\n",
      "Dask execution time: 0.38 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# --- Pandas Attempt ---\n",
    "print(\"\\n--- Timing Pandas ---\")\n",
    "start = time.time()\n",
    "df_pandas = pd.read_parquet(\"nyc_taxi_jan_2023.parquet\")\n",
    "pandas_result = df_pandas.groupby(\"payment_type\")[\"tip_amount\"].mean()\n",
    "end = time.time()\n",
    "print(\"Pandas groupby result:\")\n",
    "print(pandas_result)\n",
    "print(f\"Pandas execution time: {end - start:.2f} seconds\")\n",
    "\n",
    "# --- Dask Attempt ---\n",
    "print(\"\\n--- Timing Dask ---\")\n",
    "start = time.time()\n",
    "ddf = dd.read_parquet(\"nyc_taxi_jan_2023.parquet\")\n",
    "dask_aggregation = ddf.groupby(\"payment_type\")[\"tip_amount\"].mean()\n",
    "dask_result = dask_aggregation.compute()\n",
    "end = time.time()\n",
    "print(\"Dask groupby result:\")\n",
    "print(dask_result)\n",
    "print(f\"Dask execution time: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89224cb8-8de9-42ea-a3d3-2509b36da1c6",
   "metadata": {},
   "source": [
    "### Mini-Project Summary: Dask vs. Pandas\n",
    "\n",
    "In this project, we compared the performance of Pandas and Dask for a standard `groupby-mean` aggregation on a ~45.5 MB Parquet file of NYC taxi data.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "*   **Pandas:** Using the `time` module, we measured the time taken to load the entire Parquet file into memory and then perform the aggregation. Pandas is straightforward and fast for medium-sized datasets that fit comfortably in RAM.  \n",
    "*   **Dask:** The timing with `time` captures both the creation of the lazy task graph and the actual parallel computation triggered by `.compute()`. While Dask may not always beat Pandas for ~300 MB files, it shines when scaling to larger datasets.\n",
    "\n",
    "*Pandas execution time: 0.85 seconds*, \n",
    "*Dask execution time: 0.38 seconds*\n",
    "\n",
    "**Key Takeaways & \"Amaze Factor\" Explanation:**\n",
    "\n",
    "*   **Scalability:** Pandas handled the ~45.5 MB file fine, but it would likely fail with a `MemoryError` on a 5 GB or 50 GB dataset. Dask, by operating on partitioned chunks of the data, can seamlessly handle datasets much larger than RAM. **This is Daskâ€™s biggest advantage: out-of-core computation.**\n",
    "*   **Lazy Evaluation:** Dask builds a task graph and defers execution until `.compute()` is called. This lets it optimize the execution plan before running, reducing wasted work and enabling parallelization across cores.\n",
    "*   **Trade-offs:** For smaller files, Pandas often outperforms Dask due to Daskâ€™s task graph overhead. But as soon as datasets grow large relative to memory, Daskâ€™s parallel and chunked computation provides a massive advantage in both speed and feasibility.  \n",
    "\n",
    "ðŸ‘‰ **Final Note:** This project shows not just raw performance differences, but more importantly, the *scalability mindset* â€” knowing when to switch from Pandas to Dask as data grows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f74be-3385-46e2-b880-4040c0095314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_ds)",
   "language": "python",
   "name": "venv_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
